{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Crime Project\n",
    "\n",
    "* Author: Kevin Chuang [@k-chuang](https://www.github.com/k-chuang)\n",
    "* Created on: August 25, 2018\n",
    "* Description: Data analysis, exploration, visualization, and data mining on crime in SF\n",
    "* Original dataset: [SF Gov Crime dataset](https://data.sfgov.org/Public-Safety/-Change-Notice-Police-Department-Incidents/tmnf-yvry/about)\n",
    "* Kaggle dataset: [Kaggle SF Crime](https://www.kaggle.com/c/sf-crime/data)\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- Introduction\n",
    "    - SF Crime Dataset\n",
    "- Basic Preparation\n",
    "    - Import libraries\n",
    "    - Load data\n",
    "- Data Exploration/Analysis Extension\n",
    "- Data Preprocessing\n",
    "    - Data Imputation/Removal\n",
    "    - Feature Engineering\n",
    "    - Feature Encoding\n",
    "- Build Machine Learning Models\n",
    "    - Train different baseline models\n",
    "    - Analyze results\n",
    "- Model Selection\n",
    "- Hyperparameter tuning\n",
    "- Train Model with optimal hyperparameters\n",
    "- Feature Selection\n",
    "    - Feature Importance\n",
    "    - Feature Removal\n",
    "- Train Final Model\n",
    "- Model Evaluation\n",
    "- Summary\n",
    "- Kaggle Submission\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "## SF Crime Dataset\n",
    "\n",
    "This dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set. The goal is to try to predict the category of crime that occurred in the city of San Francisco. \n",
    "\n",
    "### Data Fields\n",
    "- **Dates** - timestamp of the crime incident\n",
    "- **Category** - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\n",
    "- **Descript** - detailed description of the crime incident (only in train.csv)\n",
    "- **DayOfWeek** - the day of the week\n",
    "- **PdDistrict** - name of the Police Department District\n",
    "- **Resolution** - how the crime incident was resolved (only in train.csv)\n",
    "- **Address** - the approximate street address of the crime incident \n",
    "- **X** - Longitude\n",
    "- **Y** - Latitude\n",
    "\n",
    "\n",
    "In this juypter notebook, I will go through the whole process, end-to-end, of creating a machine learning model on the open source San Francisco Crime dataset. This includes data exploration & analysis, data preprocessing (huge part of this project and includes feature engineering), trying out different ML algorithms and determining the optimal ML model, tuning the hyperparameters of that model, and finally, evaluating the chosen model in terms of multiclass log loss. \n",
    "\n",
    "Since this is an old Kaggle competition, I will refrain from looking online for resources or old Kaggle kernels. The plan is to get better at coding an end to end data science project and to familiarize myself with the Python data science libraries. Also, I hope to learn some interesting things and discover some cool patterns or ideas using this dataset. Well, here goes nothing!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:19.277666Z",
     "start_time": "2018-09-15T00:10:17.704742Z"
    }
   },
   "outputs": [],
   "source": [
    "__author__ = 'Kevin Chuang (https://www.github.com/k-chuang)' \n",
    "\n",
    "# linear algebra\n",
    "import numpy as np \n",
    "\n",
    "# data processing\n",
    "import pandas as pd \n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Algorithms\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "# Metrics \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Model Selection & Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space  import Real, Categorical, Integer\n",
    "\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Mathematical Functions\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:22.111001Z",
     "start_time": "2018-09-15T00:10:19.280864Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:22.435882Z",
     "start_time": "2018-09-15T00:10:22.113556Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Analysis Extension\n",
    "\n",
    "- Complete data exploration & visualizations are located in jupyter notebook: [kaggle-sf-crime-data-exploration.ipynb](kaggle-sf-crime-data-exploration.ipynb)\n",
    "- This dataset suffers from **imbalanced classes** (**TREA** has 6 occurrences while **LARCENY/THEFT** has 1,749,000 occurrences)\n",
    "    - There are a couple ways to deal with imbalanced classes, such as:\n",
    "        - Changing performance metric (Do not use accuracy, use a confusion matrix, precision, recall, F1 score, ROC curves)\n",
    "        - Resample dataset (Oversample under-represented classes, and undersample over-represented classes)\n",
    "        - Try different ML algorithms that can handle imbalanced classes\n",
    "            - Decision Trees (Random Forests/XGBoost) often perform well on imbalanced classes (due to splitting rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:22.460838Z",
     "start_time": "2018-09-15T00:10:22.438200Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:22.468060Z",
     "start_time": "2018-09-15T00:10:22.462799Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:22.772448Z",
     "start_time": "2018-09-15T00:10:22.469783Z"
    }
   },
   "outputs": [],
   "source": [
    "# set show nulls to True\n",
    "train_df.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "### Things we learned thus far:\n",
    "\n",
    "- 878,049 instances in training set (or recorded crime instances in SF)\n",
    "- 9 columns (8 potential features + 1 label (Category))\n",
    "- Data types:\n",
    "    - 2 columns with float values\n",
    "    - 7 objects\n",
    "- There are no null (NaN) values (Yay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:22.891578Z",
     "start_time": "2018-09-15T00:10:22.791301Z"
    }
   },
   "outputs": [],
   "source": [
    "## Count number of observations for each crime \n",
    "train_df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:22.992868Z",
     "start_time": "2018-09-15T00:10:22.893918Z"
    }
   },
   "outputs": [],
   "source": [
    "## Count number of observations of crime for each PD District\n",
    "train_df['PdDistrict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:23.086159Z",
     "start_time": "2018-09-15T00:10:22.995288Z"
    }
   },
   "outputs": [],
   "source": [
    "## Count number of observations for each day of week\n",
    "train_df['DayOfWeek'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:23.187282Z",
     "start_time": "2018-09-15T00:10:23.088498Z"
    }
   },
   "outputs": [],
   "source": [
    "## Count number of observations for Resolution feature\n",
    "train_df['Resolution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:23.339069Z",
     "start_time": "2018-09-15T00:10:23.189621Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[['X','Y']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There seems to be an invalid coordinates (max) 90 (latitude) or -120.5 (longitude) does not seem to be a valid coordinate in San Francisco. We must fix these values for this feature.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- Data cleaning\n",
    "    - imputation or removal of outlier values\n",
    "- Feature Engineering (Feature Creation)\n",
    "- Feature Encoding\n",
    "    - **Integer encode** or **label encode** ordinal categorical features that maintain order (Year, Business Quarter, Block/Street Number)\n",
    "    - Usually: \n",
    "        - **One hot encode** nominal categorical features (DayOfWeek, PdDistrict, StreetType, Category)\n",
    "            - mainly for logistic regression\n",
    "        - However, Random Forests & Boosting algorithms can handle nominal categorical features directly, so we just **integer encode** these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Data removal\n",
    "- Data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:23.382595Z",
     "start_time": "2018-09-15T00:10:23.341391Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[train_df['Y'] == train_df['Y'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that there are 108 rows with incorrect coordinates, and they seem to be the exact same two coordinates (90, -120.5). There are many ways to handle this. We need to do data imputation, which can be done several ways. For now, I will randomly sample from a normal distribution with the range of a standard deviation from the mean. However, I could use a linear regression model to predict the latitude and longitude values (based on other variables such as PD district?) and use that to impute the bad / inconsistent data points.\n",
    "\n",
    "Another method is to completely remove this data. Since I already have a lot of data, and I do not want this incorrect data to affect my results, I could remove them. However, I will stick with data imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:23.415432Z",
     "start_time": "2018-09-15T00:10:23.385191Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['Y'].replace(to_replace= train_df['Y'].max() ,value=np.nan, inplace=True)\n",
    "train_df['X'].replace(to_replace= train_df['X'].max() ,value=np.nan, inplace=True)\n",
    "test_df['Y'].replace(to_replace= test_df['Y'].max() ,value=np.nan, inplace=True)\n",
    "test_df['X'].replace(to_replace= test_df['X'].max() ,value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:23.917984Z",
     "start_time": "2018-09-15T00:10:23.417166Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:24.145140Z",
     "start_time": "2018-09-15T00:10:23.920445Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:24.366520Z",
     "start_time": "2018-09-15T00:10:24.147409Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    mean_X = dataset[\"X\"].mean()\n",
    "    std_X = dataset[\"X\"].std()\n",
    "    mean_Y = dataset[\"Y\"].mean()\n",
    "    std_Y = dataset[\"Y\"].std()\n",
    "    max_X = mean_X + std_X\n",
    "    min_X = mean_X - std_X\n",
    "    max_Y = mean_Y + std_Y\n",
    "    min_Y = mean_Y - std_Y\n",
    "\n",
    "    # Both X and Y will have the same null so just use Y\n",
    "    is_null = dataset['Y'].isnull().sum()\n",
    "    random_X = (max_X - min_X) * np.random.randn(is_null) + min_X\n",
    "    random_Y = (max_Y - min_Y) * np.random.randn(is_null) + min_Y\n",
    "\n",
    "    X_slice = dataset['X'].copy()\n",
    "    Y_slice = dataset['Y'].copy()\n",
    "    X_slice[np.isnan(X_slice)] = random_X\n",
    "    Y_slice[np.isnan(Y_slice)] = random_Y\n",
    "    dataset['X'] = X_slice\n",
    "    dataset['Y'] = Y_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:24.551483Z",
     "start_time": "2018-09-15T00:10:24.368883Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[['X', 'Y']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:24.558034Z",
     "start_time": "2018-09-15T00:10:24.553530Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:24.757891Z",
     "start_time": "2018-09-15T00:10:24.560297Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df[['X', 'Y']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:24.765879Z",
     "start_time": "2018-09-15T00:10:24.760906Z"
    }
   },
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- Let's create some new features from the data that exists in the current feature space\n",
    "- There are a couple categories of features:\n",
    "    - Temporal features\n",
    "    - Spatial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Features\n",
    "We want to have a column for Time, so we must parse through the 'Dates' feature to create the 'Time' feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:25.274601Z",
     "start_time": "2018-09-15T00:10:24.768516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the Date into a python datetime object.\n",
    "train_df[\"Dates\"] = pd.to_datetime(train_df[\"Dates\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "test_df[\"Dates\"] = pd.to_datetime(test_df[\"Dates\"], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:31.564182Z",
     "start_time": "2018-09-15T00:10:25.277052Z"
    }
   },
   "outputs": [],
   "source": [
    "# Minute\n",
    "train_df[\"Minute\"] = train_df[\"Dates\"].map(lambda x: x.minute)\n",
    "test_df[\"Minute\"] = test_df[\"Dates\"].map(lambda x: x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:37.693960Z",
     "start_time": "2018-09-15T00:10:31.566084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hour\n",
    "train_df[\"Hour\"] = train_df[\"Dates\"].map(lambda x: x.hour)\n",
    "test_df[\"Hour\"] = test_df[\"Dates\"].map(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:43.874460Z",
     "start_time": "2018-09-15T00:10:37.696280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Day\n",
    "train_df[\"Day\"] = train_df[\"Dates\"].map(lambda x: x.day)\n",
    "test_df[\"Day\"] = test_df[\"Dates\"].map(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:49.946466Z",
     "start_time": "2018-09-15T00:10:43.876358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Month\n",
    "train_df[\"Month\"] = train_df[\"Dates\"].map(lambda x: x.month)\n",
    "test_df[\"Month\"] = test_df[\"Dates\"].map(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:55.794134Z",
     "start_time": "2018-09-15T00:10:49.948660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Year\n",
    "train_df[\"Year\"] = train_df[\"Dates\"].map(lambda x: x.year)\n",
    "test_df[\"Year\"] = test_df[\"Dates\"].map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:10:56.787121Z",
     "start_time": "2018-09-15T00:10:55.796072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hour Zone 0 - Pass midnight, 1 - morning, 2 - afternoon, 3 - dinner / sun set, 4 - night\n",
    "def get_hour_zone(hour):\n",
    "    if hour >= 2 and hour < 8: \n",
    "        return 0\n",
    "    elif hour >= 8 and hour < 12: \n",
    "        return 1\n",
    "    elif hour >= 12 and hour < 18: \n",
    "        return 2\n",
    "    elif hour >= 18 and hour < 22: \n",
    "        return 3\n",
    "    elif hour < 2 or hour >= 22: \n",
    "        return 4\n",
    "    \n",
    "train_df[\"Hour_Zone\"] = train_df[\"Hour\"].map(get_hour_zone)\n",
    "test_df[\"Hour_Zone\"] = test_df[\"Hour\"].map(get_hour_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:03.922991Z",
     "start_time": "2018-09-15T00:10:56.789255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Week of Year\n",
    "train_df[\"WeekOfYear\"] = train_df[\"Dates\"].map(lambda x: int(x.weekofyear / 2) - 1)\n",
    "test_df[\"WeekOfYear\"] = test_df[\"Dates\"].map(lambda x: int(x.weekofyear / 2))\n",
    "\n",
    "print(sorted(train_df['WeekOfYear'].unique()))\n",
    "print(sorted(test_df['WeekOfYear'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:03.944540Z",
     "start_time": "2018-09-15T00:11:03.925176Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holiday Feature\n",
    "\n",
    "- Certain crimes may be more apparent on holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:04.369855Z",
     "start_time": "2018-09-15T00:11:03.946433Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "# Training set\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start=train_df['Dates'].min(), end=train_df['Dates'].max())\n",
    "train_df['Holiday'] = train_df['Dates'].dt.date.astype('datetime64').isin(holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:04.784067Z",
     "start_time": "2018-09-15T00:11:04.372197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test set\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start=test_df['Dates'].min(), end=test_df['Dates'].max())\n",
    "test_df['Holiday'] = test_df['Dates'].dt.date.astype('datetime64').isin(holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:04.885777Z",
     "start_time": "2018-09-15T00:11:04.786072Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_df[train_df['Holiday'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:04.993193Z",
     "start_time": "2018-09-15T00:11:04.888127Z"
    }
   },
   "outputs": [],
   "source": [
    "len(test_df[test_df['Holiday'] == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Hours Feature\n",
    "\n",
    "- There should be an effect of business hours on the type of crime committed\n",
    "- Let's create a binary feature where:\n",
    "    - 1 is typical business hours [8:00AM - 6:00PM]\n",
    "    - 0 is not business hours [6:01PM - 7:59 AM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:12.059178Z",
     "start_time": "2018-09-15T00:11:04.995075Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, time\n",
    "\n",
    "def time_in_range(start, end, x):\n",
    "    \"\"\"Return true if x is in the inclusive range [start, end]\"\"\"\n",
    "    if start <= end:\n",
    "        return start <= x <= end\n",
    "    else:\n",
    "        return start <= x or x <= end\n",
    "\n",
    "def map_business_hours(date):\n",
    "    \n",
    "    # Convert military time to AM & PM\n",
    "    time_parsed = date.time()\n",
    "    business_start = time(8, 0, 0)\n",
    "    business_end = time(18, 0, 0)\n",
    "    \n",
    "    if time_in_range(business_start, business_end, time_parsed):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "train_df['BusinessHour'] = train_df['Dates'].map(map_business_hours).astype('uint8')\n",
    "test_df['BusinessHour'] = test_df['Dates'].map(map_business_hours).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:12.078593Z",
     "start_time": "2018-09-15T00:11:12.061166Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['BusinessHour'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:12.102491Z",
     "start_time": "2018-09-15T00:11:12.082130Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season\n",
    "\n",
    "The season feature may affect what type of crimes are commited. \n",
    "- 1 = Winter, 2 = Spring, 3 = Summer, 4 = Fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:12.174037Z",
     "start_time": "2018-09-15T00:11:12.121921Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['Season']=(train_df['Month']%12 + 3)//3\n",
    "test_df['Season']=(test_df['Month']%12 + 3)//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:12.193165Z",
     "start_time": "2018-09-15T00:11:12.175930Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekend\n",
    "\n",
    "- Weekends may have effect on what types of crimes are commmited\n",
    "- Weekday = 0, Weekend =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:12.984924Z",
     "start_time": "2018-09-15T00:11:12.195523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Weekend Feature\n",
    "\n",
    "# Weekday = 0, Weekend = 1\n",
    "days = {'Monday':0 ,'Tuesday':0 ,'Wednesday':0 ,'Thursday':0 ,'Friday':0, 'Saturday':1 ,'Sunday':1}\n",
    "\n",
    "train_df['Weekend'] = train_df['DayOfWeek'].replace(days).astype('uint8')\n",
    "test_df['Weekend'] = test_df['DayOfWeek'].replace(days).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street Type\n",
    "\n",
    "The street type can have an effect on what type of crime is committed, so we want to extract the street type from the 'Address' feature.\n",
    "\n",
    "We have avenues, streets, ways, boulevards, highways, courts, walks, plazas, and differet number of intersections of roads/streets (Addresses with /)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:13.197395Z",
     "start_time": "2018-09-15T00:11:12.986765Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['Address'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:17.624158Z",
     "start_time": "2018-09-15T00:11:13.199248Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_streets(address):\n",
    "    street_types = ['AV', 'ST', 'CT', 'PZ', 'LN', 'DR', 'PL', 'HY', \n",
    "                    'FY', 'WY', 'TR', 'RD', 'BL', 'WAY', 'CR', 'AL', 'I-80',  \n",
    "                    'RW', 'WK','EL CAMINO DEL MAR']\n",
    "    street_pattern = '|'.join(street_types)\n",
    "    streets = re.findall(street_pattern, address)\n",
    "    if len(streets) == 0:\n",
    "        # Debug\n",
    "#         print(address)\n",
    "        return 'OTHER'\n",
    "    elif len(streets) == 1:\n",
    "        return streets[0]\n",
    "    else:\n",
    "#         print(address)\n",
    "        return 'INT'\n",
    "\n",
    "train_df['StreetType'] = train_df['Address'].map(find_streets)\n",
    "test_df['StreetType'] = test_df['Address'].map(find_streets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:17.759847Z",
     "start_time": "2018-09-15T00:11:17.626318Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['StreetType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:17.808128Z",
     "start_time": "2018-09-15T00:11:17.762189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "train_df['StreetType'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:17.834744Z",
     "start_time": "2018-09-15T00:11:17.810033Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Number Feature\n",
    "\n",
    "- Let's explore the block number from address\n",
    "- Block number has ordinal data type (order matters), and has spatial significance\n",
    "- It seems all the block numbers are in intervals of 100\n",
    "- How to categorize\n",
    "    - Addresses that do not have a block number will be categorized as 0\n",
    "    - Addresses with block number will be divided by 100, and added by 1 for mapping (0 is saved for addresses with no block number)\n",
    "- 85 unique block numbers (including 1 where there is no block number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:21.837626Z",
     "start_time": "2018-09-15T00:11:17.846149Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_block_number(address):\n",
    "    block_num_pattern = '[0-9]+\\s[Block]'\n",
    "    block_num = re.search(block_num_pattern, address)\n",
    "    if block_num:\n",
    "#         print(address)\n",
    "        num_pattern = '[0-9]+'\n",
    "        block_no_pos = re.search(num_pattern, address)\n",
    "        # Get integer of found regular expression\n",
    "        block_no = int(block_no_pos.group())\n",
    "        # Convert block number by dividing by 100 and adding 1 (0 = addresses with no block)\n",
    "        block_map = (block_no // 100) + 1\n",
    "#         print(block_map)\n",
    "        return block_map\n",
    "    else:\n",
    "#         print(address)\n",
    "        # \n",
    "        return 0\n",
    "\n",
    "\n",
    "train_df['BlockNo'] = train_df['Address'].map(find_block_number)\n",
    "test_df['BlockNo'] = test_df['Address'].map(find_block_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:21.875408Z",
     "start_time": "2018-09-15T00:11:21.839834Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['BlockNo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X, Y Coordinates\n",
    "\n",
    "- Normalize and scale the X and Y coordinates\n",
    "- I use **K-Means clustering** to create a new feature for the longitude and latitude by grouping clusters of points based on Euclidean distances.\n",
    "- X = longitude, Y = latitude\n",
    "- I also extract more spatial features from the X, Y coordinates by transforming them from the cartesian space to the polar space ([Reference](https://www.kaggle.com/c/sf-crime/discussion/18853))\n",
    "    1. three variants of rotated Cartesian coordinates (rotated by 30, 45, 60 degree each) \n",
    "    2. Polar coordinates (i.e. the 'r' and the angle 'theta')\n",
    "    3. The approach makes some intuitive sense i.e. that having such features should help in extracting some more spatial information (than relying on the current x-y alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:22.862516Z",
     "start_time": "2018-09-15T00:11:21.879241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize X and Y\n",
    "print('There are %d unique longitude values, %d unique latitude values' % (train_df['X'].nunique(), \n",
    "                                                                           train_df['Y'].nunique()))\n",
    "\n",
    "xy_scaler = StandardScaler().fit(train_df[['X', 'Y']])\n",
    "train_df[['X', 'Y']] = xy_scaler.transform(train_df[['X', 'Y']])\n",
    "test_df[['X', 'Y']] = xy_scaler.transform(test_df[['X', 'Y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:23.200903Z",
     "start_time": "2018-09-15T00:11:22.864443Z"
    }
   },
   "outputs": [],
   "source": [
    "# X-Y plane rotation and space transformation to extract more spatial information\n",
    "# 2-dimensional rotation based on below functions:\n",
    "# rotated x = xcos - ysin\n",
    "# rotated y = xsin + ycos\n",
    "# Conver from cartesian space -> polar space\n",
    "\n",
    "cos_30 = math.cos(math.radians(30))\n",
    "sin_30 = math.sin(math.radians(30))\n",
    "cos_45 = math.cos(math.radians(45))\n",
    "sin_45 = math.sin(math.radians(45))\n",
    "cos_60 = math.cos(math.radians(60))\n",
    "sin_60 = math.sin(math.radians(60))\n",
    "\n",
    "\n",
    "train_df[\"Rot30_X\"] = train_df['X'] * cos_30 - train_df['Y'] * sin_30 \n",
    "train_df[\"Rot30_Y\"] = train_df['X'] * sin_30 + train_df['Y'] * cos_30\n",
    "train_df[\"Rot45_X\"] = train_df['X'] * cos_45 - train_df['Y'] * sin_45  \n",
    "train_df[\"Rot45_Y\"] = train_df['X'] * sin_45 + train_df['Y'] * cos_45\n",
    "train_df[\"Rot60_X\"] = train_df['X'] * cos_60 - train_df['Y'] * sin_60  \n",
    "train_df[\"Rot60_Y\"] = train_df['X'] * sin_60 + train_df['Y'] * cos_60\n",
    "train_df[\"Radius\"] = np.sqrt(train_df['X'] ** 2 + train_df['Y'] ** 2)\n",
    "train_df[\"Angle\"] = np.arctan2(train_df['X'], train_df['Y'])\n",
    "\n",
    "test_df[\"Rot30_X\"] = test_df['X'] * cos_30 - test_df['Y'] * sin_30  \n",
    "test_df[\"Rot30_Y\"] = test_df['X'] * sin_30 + test_df['Y'] * cos_30\n",
    "test_df[\"Rot45_X\"] = test_df['X'] * cos_45 - test_df['Y'] * sin_45  \n",
    "test_df[\"Rot45_Y\"] = test_df['X'] * sin_45 + test_df['Y'] * cos_45\n",
    "test_df[\"Rot60_X\"] = test_df['X'] * cos_60 - test_df['Y'] * sin_60  \n",
    "test_df[\"Rot60_Y\"] = test_df['X'] * sin_60 + test_df['Y'] * cos_60\n",
    "test_df[\"Radius\"] = np.sqrt(test_df['X'] ** 2 + test_df['Y'] ** 2)\n",
    "test_df[\"Angle\"] = np.arctan2(test_df['X'], test_df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:11:25.125720Z",
     "start_time": "2018-09-15T00:11:23.202875Z"
    }
   },
   "outputs": [],
   "source": [
    "# View the description of the numerical features again to ensure everything is right\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:25.723811Z",
     "start_time": "2018-09-15T00:11:25.127445Z"
    }
   },
   "outputs": [],
   "source": [
    "# run KMeans separately on both the training set and test set\n",
    "data = [train_df, test_df]\n",
    "num_clusters = 40\n",
    "for dataset in data:\n",
    "    coordinates = dataset.loc[:,['Y','X']]\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=1).fit(coordinates)\n",
    "    id_labels=kmeans.labels_\n",
    "#     print(kmeans.cluster_centers_)\n",
    "    dataset['Cluster'] = id_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:25.757165Z",
     "start_time": "2018-09-15T00:15:25.727020Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Features\n",
    "\n",
    "- We have already extracted all the necessary features from the `Address` attribute, so drop\n",
    "- We don't need `Resolution` or `Descript` features since it is not included in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:26.460052Z",
     "start_time": "2018-09-15T00:15:25.759851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop Address feature from both train and test set\n",
    "train_df.drop(['Address'], axis=1, inplace=True)\n",
    "test_df.drop(['Address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:26.724329Z",
     "start_time": "2018-09-15T00:15:26.462106Z"
    }
   },
   "outputs": [],
   "source": [
    "# We don't need Dates column anymore\n",
    "train_df.drop(['Dates'], axis=1, inplace=True)\n",
    "test_df.drop(['Dates'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:26.815822Z",
     "start_time": "2018-09-15T00:15:26.726626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop Resolution column since test set does not have this column\n",
    "train_df.drop(['Resolution'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:26.912963Z",
     "start_time": "2018-09-15T00:15:26.817639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop Descript column since test set does not have this column\n",
    "train_df.drop(['Descript'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:26.936327Z",
     "start_time": "2018-09-15T00:15:26.915051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's quickly view the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Encoding \n",
    "\n",
    "- Convert categorical data to numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pd Districts\n",
    "\n",
    "- convert Pd District categorical feature to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:27.960027Z",
     "start_time": "2018-09-15T00:15:26.938164Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_districts = {'SOUTHERN':0, 'MISSION':1, 'NORTHERN':2, 'CENTRAL':3, 'BAYVIEW':4, 'INGLESIDE':5, \n",
    "                'TENDERLOIN':6, 'TARAVAL':7, 'PARK':8, 'RICHMOND':9}\n",
    "\n",
    "train_df['PdDistrict'].replace(pd_districts, inplace=True)\n",
    "test_df['PdDistrict'].replace(pd_districts, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:27.981326Z",
     "start_time": "2018-09-15T00:15:27.961850Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:28.480435Z",
     "start_time": "2018-09-15T00:15:27.983129Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year\n",
    "\n",
    "- Year is an **ordinal** variable, so let's keep that ordering and mapping\n",
    "- convert Year categorical feature to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:28.548217Z",
     "start_time": "2018-09-15T00:15:28.482183Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    year_le = LabelEncoder()\n",
    "    year_le.fit(dataset['Year'].unique())\n",
    "    print(list(year_le.classes_))\n",
    "\n",
    "    dataset['Year']=year_le.transform(dataset['Year']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:28.558184Z",
     "start_time": "2018-09-15T00:15:28.549815Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['Year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:28.565007Z",
     "start_time": "2018-09-15T00:15:28.560030Z"
    }
   },
   "outputs": [],
   "source": [
    "# So we know the mapping (important)\n",
    "dict(zip(year_le.classes_, year_le.transform(year_le.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:28.586908Z",
     "start_time": "2018-09-15T00:15:28.566585Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:28.960961Z",
     "start_time": "2018-09-15T00:15:28.588787Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DayOfWeek\n",
    "\n",
    "- we are going to use sklearn's LabelEncoder to encode the categorical data to numeric\n",
    "- Day of week is considered a categorical and nominal variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:30.490086Z",
     "start_time": "2018-09-15T00:15:28.963333Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dow_le = LabelEncoder()\n",
    "    dow_le.fit(dataset['DayOfWeek'].unique())\n",
    "    print(list(dow_le.classes_))\n",
    "    dataset['DayOfWeek']=dow_le.transform(dataset['DayOfWeek'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:30.502616Z",
     "start_time": "2018-09-15T00:15:30.491985Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['DayOfWeek'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:30.514008Z",
     "start_time": "2018-09-15T00:15:30.505862Z"
    }
   },
   "outputs": [],
   "source": [
    "# So we know the mapping (important)\n",
    "dict(zip(dow_le.classes_, dow_le.transform(dow_le.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:30.541416Z",
     "start_time": "2018-09-15T00:15:30.517311Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:31.019915Z",
     "start_time": "2018-09-15T00:15:30.543207Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street Type\n",
    "\n",
    "- we are going to use sklearn's LabelEncoder to encode the categorical data to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:32.609940Z",
     "start_time": "2018-09-15T00:15:31.021643Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    st_le = LabelEncoder()\n",
    "    st_le.fit(dataset['StreetType'].unique())\n",
    "    print(list(st_le.classes_))\n",
    "    dataset['StreetType']=st_le.transform(dataset['StreetType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:32.621448Z",
     "start_time": "2018-09-15T00:15:32.611850Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['StreetType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:32.656477Z",
     "start_time": "2018-09-15T00:15:32.631106Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:33.124717Z",
     "start_time": "2018-09-15T00:15:32.658114Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holiday\n",
    "\n",
    "- Encode the binary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:33.366955Z",
     "start_time": "2018-09-15T00:15:33.126363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode to 0 and 1\n",
    "\n",
    "train_df['Holiday'].replace(False, 0, inplace=True)\n",
    "train_df['Holiday'].replace(True, 1, inplace=True)\n",
    "test_df['Holiday'].replace(False, 0, inplace=True)\n",
    "test_df['Holiday'].replace(True, 1, inplace=True)\n",
    "\n",
    "train_df['Holiday'] = train_df['Holiday'].astype('uint8')\n",
    "train_df['Holiday'] = train_df['Holiday'].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:33.401544Z",
     "start_time": "2018-09-15T00:15:33.368756Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[train_df['Holiday'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:33.644042Z",
     "start_time": "2018-09-15T00:15:33.403046Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df[test_df['Holiday'] == 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category\n",
    "\n",
    "- we are going to use sklearn's LabelEncoder to encode the categorical data to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:34.733063Z",
     "start_time": "2018-09-15T00:15:33.645874Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [train_df]\n",
    "\n",
    "for dataset in data:\n",
    "    cat_le = LabelEncoder()\n",
    "    cat_le.fit(dataset['Category'].unique())\n",
    "    print(list(cat_le.classes_))\n",
    "    dataset['Category']=cat_le.transform(dataset['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:34.744138Z",
     "start_time": "2018-09-15T00:15:34.735515Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_df['Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:34.751446Z",
     "start_time": "2018-09-15T00:15:34.746020Z"
    }
   },
   "outputs": [],
   "source": [
    "# So we know the mapping (important)\n",
    "dict(zip(cat_le.classes_, cat_le.transform(cat_le.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:34.771176Z",
     "start_time": "2018-09-15T00:15:34.753364Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:35.216673Z",
     "start_time": "2018-09-15T00:15:34.772873Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Information about Data\n",
    "\n",
    "- One last check before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:35.519400Z",
     "start_time": "2018-09-15T00:15:35.218326Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:36.424967Z",
     "start_time": "2018-09-15T00:15:35.520949Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Convert all to 32 bit integers so less memory and will train faster (no loss in data since our integers dont reach)\n",
    "columns_to_convert = ['DayOfWeek', 'PdDistrict', 'Minute', 'Hour', 'Day', 'Month', 'Year', \n",
    "                      'Hour_Zone', 'WeekOfYear', 'Season', 'StreetType', 'BlockNo', 'Cluster']\n",
    "train_df[columns_to_convert] = train_df[columns_to_convert].astype('int16')\n",
    "test_df[columns_to_convert] = test_df[columns_to_convert].astype('int16')\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Models\n",
    "\n",
    "- Baseline Models\n",
    "    - Let's train a couple models on a stratified sample of the training data\n",
    "    - Evaluate on a hold out set to get baseline results for each model to determine what model to use\n",
    "    - Models:\n",
    "        - Stochastic Gradient Descent (with elastic net regularization)\n",
    "        - Gaussian Naive Bayes\n",
    "        - K Nearest Neighbors\n",
    "        - Logistic Regression (with L1 regularization)\n",
    "        - Random Forest\n",
    "        - XGBoost\n",
    "    - Almost all the default scikit-learn ML algorithm hyperparameters exhibit bad performance\n",
    "        - Researched online & read literature to determine some more ideal default hyperparameters\n",
    "            - [Reference](https://arxiv.org/abs/1708.05070)\n",
    "- Couple things to note:\n",
    "    - **Decision tree models** including Ensemble methods (Random Forest & XGBoost) can handle categorical variables without one-hot encoding them. \n",
    "    - **Linear models** (SGD & Logistic Regression) cannot handle categorical features & need features to be OHE before training\n",
    "    - Always OneHotEncode before you split data up to training/dev/test so that all features & classes will be represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:36.648460Z",
     "start_time": "2018-09-15T00:15:36.426939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set training data (drop labels) and training labels\n",
    "X_train = train_df.drop(\"Category\", axis=1).copy()\n",
    "Y_train = train_df[\"Category\"].copy()\n",
    "\n",
    "# Set testing data (drop Id)\n",
    "X_test = test_df.drop(\"Id\", axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T00:15:39.373300Z",
     "start_time": "2018-09-15T00:15:38.891328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use these for ML algorithms that can handle categorical data without OHE\n",
    "mini_train_data, mini_dev_data, mini_train_labels, mini_dev_labels = train_test_split(X_train, \n",
    "                                                                                      Y_train,\n",
    "                                                                                      stratify=Y_train,\n",
    "                                                                                      test_size=0.5,\n",
    "                                                                                      random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "- Hyperparameter tuning involves defining an objective function (log loss), and using cross-validation to measure the hyperparameter quality. \n",
    "    - We want the hyperparameters that give the highest generalization performance.\n",
    "- Three approaches: Grid Search (`GridSearchCV`), Random Search (`RandomSearchCV`), and Bayes Optimization (`BayesSearchCV`)\n",
    "- Realized `GridSearchCV` took way too long and was impractical, and `RandomSearchCV` was too random.\n",
    "    - Grid and random search are completely uninformed by past evaluations, and as a result, often spend a significant amount of time evaluating bad hyperparameters.\n",
    "- Then, I did more research on more efficient & smarter hyperparameter tuning techniques and found Bayeisan Optimization (`BayesSearchCV`)\n",
    "- **Bayesian Optimization Overview**\n",
    "    - Build a probabilistic model of the objective function & use it to select promising hyperparameters to evaluate in the true objective function\n",
    "        - The model used for approximating the objective function is called *surrogate model*. \n",
    "            - E.g. Gaussian Processes \n",
    "    - Keeps track of past evaluation results, which is used to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function\n",
    "    - Instead of optimizing an expensive objective function, we optimize on a cheap proxy function instead.\n",
    "        - *Acquisition function* that directs sampling to areas where an improvement over the current best observation is likely.\n",
    "            - E.g. maximum probability of improvement (MPI), expected improvement (EI) and upper confidence bound (UCB)\n",
    "- **K-Folds Cross Validation**\n",
    "    - Use cross validation to measure the true generalization performance of a model \n",
    "    - This is integrated with the hyperparameter tuning techniques (`GridSearchCV`, `RandomSearchCV`, `BayesSearchCV`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (Boosting)\n",
    "\n",
    "- Basic Overview:\n",
    "    - Another ensemble method that uses Boosting instead of Bagging (Random Forests)\n",
    "    - In **Boosting**, the trees are built sequentially such that each subsequent tree aims to reduce the errors of the previous tree.\n",
    "    - Each tree learns from its predecessors and updates the residual errors. \n",
    "    - Each base learner is weak (high bias) and contributes some vital information for prediction, enabling the boosting technique to produce a strong learner by effectively combining these weak learners.\n",
    "    - The final strong learner brings down both the **bias** and the **variance**.\n",
    "    - In contrast to bagging techniques like Random Forest, in which trees are grown to their maximum extent, boosting makes use of trees with fewer splits\n",
    "        -  Such small trees, which are not very deep, are **highly interpretable**. \n",
    "- Basic Steps:\n",
    "    1. Initial model `F0` to predict target variable `y`. Used to also calculate residual (`y - F0`)\n",
    "    2. A new model `h1` is used to fit to the residuals from the previous step\n",
    "    3. Now, `F0` and `h1` are combined to give `F1`, which is the boosted version of `F0`. \n",
    "        - The MSE or whatever cost function you use (Log loss, MAE) of `F1` will be lower than `F0`.\n",
    "    4. Iterate the above steps to create new models based off the previous models.\n",
    "    \n",
    "### Prevent Overfitting:\n",
    "- Large number of trees will cause overfitting (unlike Random Forests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with optimal hyperparameters & all features\n",
    "\n",
    "- Initially, I started with a Random Forest, but decided to use XGBoost\n",
    "- We first train the model (with all the features) using the optimal hyperparameters that were found through `BayesSearchCV`\n",
    "- Then, I use the model to predict the probabilities of test set with all the features\n",
    "    - I'll save these predictions later to compare them with another model I will train with certain features removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T08:00:30.846347Z",
     "start_time": "2018-09-17T05:06:43.255143Z"
    }
   },
   "outputs": [],
   "source": [
    "# It seems running time scales quadratically with the number of classes\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=86, \n",
    "    objective=\"multi:softprob\", \n",
    "    learning_rate=0.1858621466840661,\n",
    "    colsample_bylevel=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    gamma=0.49999999999999994,\n",
    "    max_delta_step=0,\n",
    "    max_depth=50,\n",
    "    min_child_weight=5,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=60.121460571845695,\n",
    "    scale_pos_weight=1e-06,\n",
    "    subsample=1.0,\n",
    "    random_state=1, \n",
    "    tree_method=\"gpu_hist\",\n",
    "    gpu_id=0,\n",
    "    n_jobs=4,\n",
    "    verbosity=0)\n",
    "\n",
    "\n",
    "xgb.fit(X_train, Y_train)\n",
    "\n",
    "Y_test_pred = xgb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "- Measured by mean decrease in Gini information\n",
    "- This is a form of feature selection that ensemble methods (Random Forest, XGBoost) can use to prevent overfitting\n",
    "    - I drop the features that seem unimportant & with less than a 1% contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T08:00:40.093635Z",
     "start_time": "2018-09-17T08:00:30.849884Z"
    }
   },
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'feature': X_train.columns,\n",
    "                            'importance': np.round(xgb.feature_importances_, 5)})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T08:00:40.175114Z",
     "start_time": "2018-09-17T08:00:40.097249Z"
    }
   },
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "- Evaluate final model based on K-Fold cross validation\n",
    "- Average all K iterations to give the true estimate of the final model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T04:41:05.199240Z",
     "start_time": "2018-09-02T04:27:11.535Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(xgb, X_train, Y_train, \n",
    "                         cv=StratifiedKFold(n_splits=5), \n",
    "                         scoring = \"neg_log_loss\", n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T04:41:05.202180Z",
     "start_time": "2018-09-02T04:27:11.537Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Scores:\", scores)\n",
    "print(\"Mean:\", scores.mean())\n",
    "print(\"Standard Deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Submission\n",
    "\n",
    "- Reformat and turn in predictions and results from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T10:48:47.627937Z",
     "start_time": "2018-09-17T10:48:47.623352Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T11:06:50.148631Z",
     "start_time": "2018-09-17T11:06:47.249150Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T11:06:50.154768Z",
     "start_time": "2018-09-17T11:06:50.150430Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T11:06:54.120643Z",
     "start_time": "2018-09-17T11:06:50.157476Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_submission.iloc[:, 1:] = pd.DataFrame(Y_test_pred, columns=sample_submission.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T11:06:54.159292Z",
     "start_time": "2018-09-17T11:06:54.122433Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T11:07:37.190126Z",
     "start_time": "2018-09-17T11:06:54.161512Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv('data/submission_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- After lots of tuning, I finally achieved a kaggle evaluation score (multiclass log loss) of **2.25674**, which would ideally rank at **#136** (out of 2,335 teams) or at the **top 6%** or **94th percentile** on the public leaderboard\n",
    "    - Since this is an old kaggle competition, this would most likely be a lower rank, but I still felt proud to achieve this score\n",
    "    - It is possible that I could run more experiments and tune the hyperparameters to achieve an even better score & ranking \n",
    "    - This was more of a learning experience for me & to get my feet wet with Data Science projects & Kaggle competitions\n",
    "    - In an effort to learn, I refrained from looking up old Kaggle kernels & other resources that completed this specific Kaggle competition.\n",
    "    - I coded most of this myself to learn the data science libraries, but did use resources such as other Kaggle competition kernels and research papers to get a better idea of how to think about the data. Google is awesome.\n",
    "- Below, I show images of my two highest scoring submissions on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This project has taught me a lot about data science and has given me hands-on experience with working with data and completing an end-to-end data science project. I've had a lot of fun visualizing, analyzing, and experimenting with the data to gain more insight. This is just the beginning of my journey into data science, and I am very excited to see what the future holds in terms of new and interesting data science problems and datasets.\n",
    "\n",
    "- **What I learned**:\n",
    "    - There are more efficient ways to label or integer encode features\n",
    "        - Will use sklearn's LabelEncoder, OneHotEncoder, & MultiLabelBinarizer next time\n",
    "    - Instead of just blindly training models, research more about ways to optimize the hyperparameters efficiently\n",
    "        - Spent too many AWS EC2 hours with `GridSearchCV`, when I should have used *Bayesian Optimization* for efficient hyperparameter tuning\n",
    "        - Do more research on the domain of the problem, certain core ML algorithms, and data processing techniques\n",
    "- **What's next?**\n",
    "    - AutoML with `tpot` or `auto-sklearn`\n",
    "        - automate the hyperparameter tuning and model selection with AutoML packages\n",
    "    - Problem Redirection (Classification ---> Regression)\n",
    "        - Instead of predicting category of crime, predict X & Y coordinates (longitude & latitude) continuous values given same spatial and temporal features as well as category of crime\n",
    "        - **Use case:** Dynamically concentrate police on certain serious categories of crime to prevent crimes from happening beforehand\n",
    "    - Rewrite all code in the jupyter notebook to .py files\n",
    "        - Modularize each of the steps with functions and/or classes\n",
    "        - Useful because I can run the .py file on AWS EC2 without having to host it on jupyter notebook locally\n",
    "            - Meaning I can peacefully shut down my laptop and let script run in the cloud overnight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 12/9 \"\"\"\n",
    "\n",
    "cross_valid_result = []\n",
    "valid_data_size = int(np.ceil(X_train.values.shape[0]/5))\n",
    "\n",
    "for i in range(5):\n",
    "    s, e = valid_data_size*i, valid_data_size*(i+1)\n",
    "    xgb = XGBClassifier(\n",
    "        early_stopping_rounds=10,\n",
    "        n_estimators=300, \n",
    "        objective=\"multi:softprob\", \n",
    "        learning_rate=0.1858621466840661,\n",
    "        colsample_bylevel=1.0,\n",
    "        colsample_bytree=1.0,\n",
    "        gamma=0.49999999999999994,\n",
    "        max_delta_step=0,\n",
    "        max_depth=50,\n",
    "        min_child_weight=5,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=60.121460571845695,\n",
    "        scale_pos_weight=1e-06,\n",
    "        subsample=1.0,\n",
    "        random_state=1, \n",
    "        n_jobs=4,\n",
    "        tree_method='gpu_hist', \n",
    "        gpu_id = 0,\n",
    "        silent=False)\n",
    "    X = pd.concat([X_train[:s],X_train[e:]])\n",
    "    Y = pd.concat([Y_train[:s],Y_train[e:]])\n",
    "\n",
    "    xgb.fit(X, Y, eval_set=[(X_train[s:e], Y_train[s:e])])\n",
    "\n",
    "    cross_valid_result.append(xgb.predict_proba(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rody-V7qEFACp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7009de0d9fea8a1bb9139c1b5b2565f530db67bfa6662b9b73c43cc08abad827"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
